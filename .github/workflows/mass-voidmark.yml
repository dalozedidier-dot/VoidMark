name: mass-voidmark

on:
  workflow_dispatch:
    inputs:
      runs:
        description: "Nombre d'itérations (ex 200)"
        required: false
        default: "200"
      n:
        description: "Taille du payload synthétique"
        required: false
        default: "200"
      seed_base:
        description: "Base seed (seed = seed_base + i)"
        required: false
        default: "5000"
      keep_outputs:
        description: "Conserver les vaults complets (true/false)"
        required: false
        default: "false"

concurrency:
  group: voidmark-mass-${{ github.ref }}
  cancel-in-progress: true

jobs:
  mass:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Ensure .github/constraints.txt exists (generated if missing)
        run: |
          set -euo pipefail
          if [ -f .github/constraints.txt ]; then
            echo ".github/constraints.txt present"
            exit 0
          fi
          mkdir -p .github
          cat > .github/constraints.txt << 'EOF'
          # Pins optionnels pour pip (ne s'installent pas "tout seuls")
          jsonschema==4.23.0
          pytest==8.3.3
          packaging==24.1
          typing-extensions==4.12.2
          EOF
          sed -n '1,200p' .github/constraints.txt

      - name: Install deps (pinned via .github/constraints.txt)
        run: |
          set -euo pipefail
          python -m pip install -U pip
          if [ -f requirements.txt ] && [ -s requirements.txt ]; then
            pip install -r requirements.txt -c .github/constraints.txt
          else
            echo "requirements.txt absent ou vide, skip"
          fi
          if [ -f requirements-dev.txt ] && [ -s requirements-dev.txt ]; then
            pip install -r requirements-dev.txt -c .github/constraints.txt
          else
            echo "requirements-dev.txt absent ou vide, skip"
          fi

      - name: Set PYTHONPATH (root + src if present)
        run: |
          set -euo pipefail
          echo "PYTHONPATH=${GITHUB_WORKSPACE}:${GITHUB_WORKSPACE}/src" >> "$GITHUB_ENV"

      - name: Mass marks (no numpy/pandas)
        env:
          RUNS: ${{ github.event.inputs.runs }}
          N: ${{ github.event.inputs.n }}
          SEED_BASE: ${{ github.event.inputs.seed_base }}
          KEEP: ${{ github.event.inputs.keep_outputs }}
        run: |
          set -euo pipefail
          mkdir -p _ci_out/mass
          python - << 'PY'
          import os, json, subprocess, hashlib, shutil, csv, random
          from pathlib import Path

          runs = int(os.environ["RUNS"])
          n = int(os.environ["N"])
          seed_base = int(os.environ["SEED_BASE"])
          keep = (os.environ.get("KEEP","false").strip().lower() == "true")

          out_root = Path("_ci_out/mass")
          out_root.mkdir(parents=True, exist_ok=True)

          def run_cmd(cmd):
            p = subprocess.run(cmd, capture_output=True, text=True)
            if p.returncode != 0:
              raise RuntimeError("cmd_failed\ncmd=%s\nstdout=\n%s\nstderr=\n%s" % (" ".join(cmd), p.stdout, p.stderr))

          def sha256_file(p: Path) -> str:
            h = hashlib.sha256()
            with p.open("rb") as f:
              for chunk in iter(lambda: f.read(1024*1024), b""):
                h.update(chunk)
            return h.hexdigest()

          rows = []
          for i in range(1, runs + 1):
            seed = seed_base + i
            rng = random.Random(seed)

            run_dir = out_root / f"run_{i:04d}"
            run_dir.mkdir(parents=True, exist_ok=True)

            payload = {
              "tool": "voidmark.mass",
              "run": i,
              "seed": seed,
              "values": [rng.gauss(0.0, 1.0) for _ in range(n)],
            }
            artifact = run_dir / "artifact.json"
            artifact.write_text(json.dumps(payload, indent=2), encoding="utf-8")

            vault = run_dir / "vault"
            vault.mkdir(parents=True, exist_ok=True)

            run_cmd(["python", "-m", "voidmark", str(artifact), "--vault-dir", str(vault)])

            chain = vault / "vault_chain.json"
            block1 = vault / "block_1_artifact.json"

            rows.append({
              "run": i,
              "seed": seed,
              "n": n,
              "artifact_sha256": sha256_file(artifact),
              "vault_chain_sha256": sha256_file(chain) if chain.exists() else "",
              "block1_sha256": sha256_file(block1) if block1.exists() else "",
              "vault_files_count": sum(1 for _ in vault.rglob("*") if _.is_file()),
            })

            if not keep:
              keep_dir = run_dir / "vault_keep"
              keep_dir.mkdir(parents=True, exist_ok=True)
              if chain.exists():
                (keep_dir / "vault_chain.json").write_text(chain.read_text(encoding="utf-8"), encoding="utf-8")
              if block1.exists():
                (keep_dir / "block_1_artifact.json").write_text(block1.read_text(encoding="utf-8"), encoding="utf-8")
              shutil.rmtree(vault, ignore_errors=True)

          summary_csv = out_root / "summary.csv"
          with summary_csv.open("w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=list(rows[0].keys()) if rows else ["run"])
            w.writeheader()
            for r in rows:
              w.writerow(r)

          (out_root / "summary.json").write_text(json.dumps(rows, indent=2), encoding="utf-8")
          print("wrote", summary_csv.resolve())
          PY

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: voidmark-mass
          path: _ci_out/mass/**
